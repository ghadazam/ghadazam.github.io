<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Open-CADBIN</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons 
  <link href="img/favicon.png" rel="icon">
  <link href="img/apple-touch-icon.png" rel="apple-touch-icon"> -->

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">
  <link href="lib/venobox/venobox.css" rel="stylesheet">
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- =======================================================
    Theme Name: TheEvent
    Theme URL: https://bootstrapmade.com/theevent-conference-event-bootstrap-template/
    Author: BootstrapMade.com
    License: https://bootstrapmade.com/license/
  ======================================================= -->
</head>

<body>

  <!--==========================
    Header
  ============================-->
  <header id="header" style="background-color: black">
    <div class="container">

      <div id="logo" class="pull-left">
        <!-- Uncomment below if you prefer to use a text logo -->
        <!-- <h1><a href="index.html">OchaLBI<span>2020</span></a></h1> -->
      </div>

      <nav id="nav-menu-container">
       <ul class="nav-menu">
          <li class="menu-active"><a href="index.html#intro">Home</a></li>
          <li><a href="index.html#about">About</a></li>
          <li><a href="index.html#speakers">Speakers</a></li>
          <li><a href="index.html#schedule">Schedule</a></li>
          <li><a href="Talks.html">Talks</a></li>
          <li><a href="Committee.html">Organizers</a></li>
          <li class="EMBC2020"><a href="https://embc.embs.org/2020/">EMBS 2020</a></li>
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->
  
  <section> <div class="container"> <br> <br> <br> <br> </div> </section>

  
    <!--==========================
     Call for Papers Section
    ============================-->
    <section id="submission" class="wow fadeInUp">
	<br/><br/><br/>
	<div class="section-header">
          <h2> Titles & Abstracts </h2>
          <p></p>
        </div>
	    
	  <img src="ghadazam.github.io/img/dg.jpg" style="float:left; margin:45px; border-radius:20%;">    
	    <b><a><h3> <b>Title: </b> Explaining Deep Learning Using Radiologist Defined Semantic Features </h3></a>
		    <a href="https://aivi.cse.usf.edu/~goldgof/"><h3><b>Speaker:</b> Dmitry Goldgof, University of South Florida </h3></a> <p></b>
   	   <p style="text-align:left;"> 
		   Quantitative features are generated from a tumor phenotype by various data characterization and feature extraction approaches. These features give us information about a nodule e.g., nodule size, pixel intensity, histogram based information, and texture information from wavelets or a convolution kernel. Semantic features, on the other hand, can be generated by an experienced radiologist and consist of the common characteristics of a tumor e.g. location of a tumor, fissure or pleural wall attachment, presence of fibrosis or emphysema, concave cut on nodule surface etc. Semantic features have also shown promise in predicting malignancy. Deep features from images are generally extracted from the last layers before the classification layer of a convolutional neural network (CNN). These networks have strong capability in learning specific patterns and textures from different types of images. However, the features extracted by CNN cannot be easily explained (black box) since they are just column numbers or positions of neurons in the hidden layers. In this talk, we propose a new approach to explanability of deep features via semantic and quantitative features. Specifically, we discuss how traditional quantitative features and semantic features can be used to relate and explain deep features. We also show how twenty-six deep features from the Vgg-S neural network and twelve deep features from our trained CNN could be explained by semantic or traditional quantitative features.  The proposed approach, which can be applied to enhance explanability of various medical image applications, shows promise toward transparent, understandable, and explainable decision-making.
	    </p>
		   <br/>
	      <hr>
         <br/>
     	 <img src="ghadazam.github.io/img/jp.jpg" style="float:left; margin:45px; border-radius:20%;">         
	    <b><a><h3> <b>Title: </b> Lessons learned from Machine Learning in Google applied to Medical and Biological Imaging</h3></a>
		 <a href="https://www.healthit.gov/hitac/member/ming-jack-po"><h3><b>Speaker:</b> Ming Jack Po, Google </h3></a> <p></b>
 	 <p style="text-align:left;"> 
	    In March of 2016, the AlphaGo computer program beat world champion (and human) Lee Sedol at the board game Go. The program's success reflected the significant progress that machine learning research has made in recent years. However, AlphaGo was just one example of what can be achieved with machine learning. This talk will provide an overview of some of the techniques that are being used in machine learning today, as well as some recent and ongoing work by Google's research teams to advance the applications of machine learning, particularly its role in biomedical research.  The talk will also discuss some of the unique challenges around applications in healthcare.
	    </p>
	    <br/><br/><br/><br/>
	    <hr>
         <br/>

        <img src="ghadazam.github.io/img/XueZhiyun.jpg" style="float:left; margin:45px; border-radius:20%;">         
	    <b><a><h3><b>Title: </b>Automated Visual Evaluation (AVE) for Cervical Cancer Screening and its Challenges </h3></a>
		    <a href="https://www.nlm.nih.gov/index.html"><h3><b>Speaker: </b>Zhiyun (Jaylene) Xue , National Library of Medicine </h3></a> <p></b>
	<p style="text-align:left;"> 
		Cervical cancer is one of the leading gynecologic diseases that affects the life of many women worldwide especially in low- and medium-resource regions (LMRR). Regular screening and early diagnosis and treatment play a critical role in the prevention of cervical cancer. Visual inspection with acetic acid (VIA) is an inexpensive screening approach commonly used in LMRR, but has been shown to be inaccurate. In this talk, we present our work on utilizing deep learning techniques to automatically evaluate the visual appearance of the acetowhitened uterine cervix using mobile devices for the goal of assisting or maybe ultimately replacing VIA. We face well-known challenges that impact use of deep learning for medical imaging applications, such as, having a small amount of labeled data or having a very unbalanced datasets. But, there are also additional practical challenges that need to be addressed for using AVE in real world, such as, the control of image quality and its influence on the AVE algorithm, the robustness of the algorithm across multiple devices, and the adjustment of AVE for variability in cervix appearance in different geographical regions. The talk will describe our findings and challenges that guide next steps for research in the field. 
	    </p>
	     <br/> <br/>
	    <hr>
	 <br/>
	    
	 <img src="ghadazam.github.io/img/sr.jpg" style="float:left; margin:45px; border-radius:20%;">         
	    <b><a><h3><b>Title: </b>Deep neural ensembles for improved pulmonary abnormality detection in chest radiographs </h3></a>
		    <a href="https://lhncbc.nlm.nih.gov/personnel/sivaramakrishnan-rajaraman"><h3> <b>Speaker: </b>Sivaramakrishnan Rajaraman, National Library of Medicine </h3></a> <p></b>
	<p style="text-align:left;"> 
		Cardiopulmonary diseases account for a significant proportion of deaths and disabilities across the world. Chest X-rays are a common diagnostic imaging modality for confirming intra-thoracic cardiopulmonary abnormalities. However, there remains an acute shortage of expert radiologists, particularly in under-resourced settings that results in interpretation delays and could have global health impact. These issues can be mitigated by an artificial intelligence (AI) powered computer-aided diagnostic (CADx) system. Such a system could help supplement decision-making and improve throughput while preserving and possibly improving the standard-of-care. A majority of such AI-based diagnostic tools at present use data-driven deep learning (DL) models that perform automated feature extraction and classification. Convolutional neural networks (CNN), a class of DL models, have gained significant research prominence in tasks related to image classification, detection, and localization. The literature reveals that they deliver promising results that scale impressively with an increasing number of training samples and computational resources. However,  the techniques may be adversely impacted due to their sensitivity to high variance or fluctuations in training data. Ensemble learning helps mitigate these by combining predictions  and blending intelligence from multiple learning algorithms. Complex non-linear functions constructed within ensembles help improve robustness and generalization. Empirical result predictions have demonstrated superiority over the conventional approach with stand-alone CNN models. In this talk, I will describe example work at the NLM that use model ensembles to improve pulmonary abnormality detection in chest radiographs.
	    </p>
	     <br/> <br/>
	    <hr>
         <br/>
	    
	<img src="ghadazam.github.io/img/al.jpg" style="float:left; margin:40px; border-radius:20%;">    
	    <b><a><h3><b>Title: </b> TBA </h3></a>
		    <b><a href="https://bme.columbia.edu/faculty/andrew-laine" target="_blank"><h3><b>Speaker: </b> Andrew Laine, Columbia University</h3></a>
	<p></b>
		    <p style="text-align:left;"> TBA   </p>	
		    <br/> <br/> <br/><br/> <br/> <br/><br/> <br/> <br/><br/> <br/><br/> <br/><br/> <br/><br/> <br/>
	       <hr>
	  <br/> 
		    
	 <img src="ghadazam.github.io/img/fs.jpeg" style="float:left; margin:40px; border-radius:20%;">         
		    <b><a><h3><b>Title: </b>Intel Products for AI-based Healthcare Applications</h3></a>
	<a href="https://www.linkedin.com/in/fillipe-souza-a8281820" target="_blank"><h3><b>Speaker: </b> Fillipe de Souza, Intel </h3></a> <p></b>
		    <p style="text-align:left;"> 
			    This talk will present Intel’s software and hardware products that are publicly available for prototyping and deployment in artificial intelligent (AI) applications. Deep learning is in the mainstream of algorithms to build most applications for they have vastly enabled great strides towards their practicality across multiple domains, for which functional requirements in terms of accuracy and computing performance were once encumbered due to the limitations of data availability and computing power. Today, there has been rapid growth in designing and manufacturing ASICs to help CPU-based hosts to accelerate NNs-based workloads. In this context, we will present Intel solutions from different computing environment perspectives, spawning from the Edge through the Cloud to FPGA-based acceleration. We will discuss how the target applications can be modeled, prototyped, and optimized for Intel CPUs and accelerators. Then, we will describe how to make the right choice of Intel hardware given latency, portability, power, cost, and other requirements demanded by the target application. We then will introduce Intel OpenVINO Toolkit, which optimizes AI workloads initially implemented and trained on CPUs/GPUs using popular deep learning frameworks (e.g., TensorFlow) and run inference efficiently on Intel processors and co-processors. We will provide a brief overview on Intel pretrained deep learning models that demonstrate the broadness of supported topologies available to get one started with developing their own applications. Finally, we will further expand our discussion on Intel’s portfolio for AI-based Healthcare applications. 
		    </p>
		    <br/> <br/> <br/><br/> 
	       <hr>
         <br/> <br/> <br/>
  
          <div class="col-lg-9">
		 
    </section>

	<br><br>
	
	<!--==========================
    Footer
  ============================-->
  <footer id="footer">
    <div class="footer-top">
      <div class="container">
        <div class="row">
		<!--
          <div class="col-lg-3 col-md-6 footer-info">
            <img src="img/logo.png" alt="TheEvenet">
            <p>In alias aperiam. Placeat tempore facere. Officiis voluptate ipsam vel eveniet est dolor et totam porro. Perspiciatis ad omnis fugit molestiae recusandae possimus. Aut consectetur id quis. In inventore consequatur ad voluptate cupiditate debitis accusamus repellat cumque.</p>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>-->

         <div class="col-lg-12 col-md-6 footer-contact">
	    <center> <h4>Questions?</h4> </center>
            <p>
            <center> <b>Contact: Ghada Zamzmi</b></center>
            <center> <b>alzamzmiga AT mail DOT nih DOT gov</b></center>
            </p>
           

          </div>

        </div>
      </div>
    </div>

    <div class="container">
      <div class="copyright">
        &copy; 2019-2020 <strong>NLM/EMBC2020 </strong> All Rights Reserved.
      </div>
      <div class="credits">
        <!--
          All the links in the footer should remain intact.
          You can delete the links only if you purchased the pro version.
          Licensing information: https://bootstrapmade.com/license/
          Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=TheEvent
        
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a> -->
      </div>
    </div>
  </footer><!-- #footer -->

   

  <a href="#" class="back-to-top"><i class="fa fa-angle-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/venobox/venobox.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>
</body>

</html>

