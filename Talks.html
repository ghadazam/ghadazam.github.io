<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Open-CADBIN</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons 
  <link href="img/favicon.png" rel="icon">
  <link href="img/apple-touch-icon.png" rel="apple-touch-icon"> -->

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">
  <link href="lib/venobox/venobox.css" rel="stylesheet">
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- ======================================================
    Theme Name: TheEvent
    Theme URL: https://bootstrapmade.com/theevent-conference-event-bootstrap-template/
    Author: BootstrapMade.com
    License: https://bootstrapmade.com/license/
  ======================================================= -->
</head>

<body>

  <!--==========================
    Header
  ============================-->
  <header id="header" style="background-color: black">
    <div class="container">

      <div id="logo" class="pull-left">
        <!-- Uncomment below if you prefer to use a text logo -->
        <!-- <h1><a href="index.html">OchaLBI<span>2020</span></a></h1> -->
      </div>

      <nav id="nav-menu-container">
       <ul class="nav-menu">
          <li class="menu-active"><a href="index.html#intro">Home</a></li>
          <li><a href="index.html#about">About</a></li>
          <li><a href="index.html#speakers">Speakers</a></li>
          <li><a href="index.html#schedule">Schedule</a></li>
          <li><a href="Talks.html">Talks</a></li>
          <li><a href="Committee.html">Organizers</a></li>
          <li class="EMBC2020"><a href="https://embc.embs.org/2020/">EMBS 2020</a></li>
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->
  
  <section> <div class="container"> <br> <br> <br> <br> </div> </section>

  
    <!--==========================
     Call for Papers Section
    ============================-->
     <!--==========================
      Speakers Section
    ============================-->
    <section id="speakers" class="wow fadeIn">
      <div class="container">
        <div class="section-header">
          <h2>Titles and Abstracts</h2>
          <p></p>
        </div>
		
		<hr>

     <img src="ghadazam.github.io/img/al.jpg" style="float:left; margin:40px; border-radius:20%;">    
	    <b><a><h3><b>Title: </b> Unsupervised Learning of Localized Texture Patterns of Pulmonary Emphysema on Computed Tomography </h3></a>
		    <b><a href="https://bme.columbia.edu/faculty/andrew-laine" target="_blank"><h3><b>Speaker: </b> Andrew Laine, Columbia University</h3></a>
	<p></b>
		    <p style="text-align:left;"> 
   </p>	
Computed tomography (CT) imaging enables in vivo assessment of lung parenchyma and several lung diseases. CT scans are key for the diagnosis of 1) chronic obstructive pulmonary disease (COPD), which is the fourth leading cause of death worldwide, and overlaps with pulmonary emphysema. We propose an original unsupervised approach to learn emphysema-specific radiological texture patterns. We have designed dedicated spatial and texture features and a two-stage learning strategy incorporating clustering and graph partitioning. Learning was performed on a cohort of 2,922 high-resolution CT scans, which included a high prevalence of smokers and COPD subjects. Experiments lead to discovering 10 highly-reproducible spatially-informed lung texture patterns and 6 quantitative emphysema subtypes (QES). Our discovered QES were associated independently with distinct risk of symptoms, physiologic changes, exacerbations and mortality. Genome-wide association studies identified loci associated with four subtypes, one with compelling functional evidence. We then developed a deep-learning network, using unsupervised domain adaptation with adversarial training, to label the QES on cardiac CT scans, which included 2/3rds of the lung. Our proposed method accounted for acquisition differences in CT image quality, and enabled us to study the progression of QES on a cohort of 17,039 longitudinal cardiac CT scans. The discovered QES provide novel emphysema subphenotyping that may facilitate future study of emphysema development, understanding the stages of COPD and the design of personalized therapies for treatment.		  <br/> 
	<br/><br/>   
	 <hr>     
		    
	    <img src="ghadazam.github.io/img/dg.jpg" style="float:left; margin:45px; border-radius:20%;">    
	    <b><a><h3> <b>Title: </b> Explaining Deep Learning Using Radiologist Defined Semantic Features </h3></a>
		    <a href="https://aivi.cse.usf.edu/~goldgof/"><h3><b>Speaker:</b> Dmitry Goldgof, University of South Florida </h3></a> <p></b>
   	   <p style="text-align:left;"> 
		   Quantitative features are generated from a tumor phenotype by various data characterization and feature extraction approaches. These features give us information about a nodule e.g., nodule size, pixel intensity, histogram based information, and texture information from wavelets or a convolution kernel. Semantic features, on the other hand, can be generated by an experienced radiologist and consist of the common characteristics of a tumor e.g. location of a tumor, fissure or pleural wall attachment, presence of fibrosis or emphysema, concave cut on nodule surface etc. Semantic features have also shown promise in predicting malignancy. Deep features from images are generally extracted from the last layers before the classification layer of a convolutional neural network (CNN). These networks have strong capability in learning specific patterns and textures from different types of images. However, the features extracted by CNN cannot be easily explained (black box) since they are just column numbers or positions of neurons in the hidden layers. In this talk, we propose a new approach to explanability of deep features via semantic and quantitative features. Specifically, we discuss how traditional quantitative features and semantic features can be used to relate and explain deep features. We also show how twenty-six deep features from the Vgg-S neural network and twelve deep features from our trained CNN could be explained by semantic or traditional quantitative features.  The proposed approach, which can be applied to enhance explanability of various medical image applications, shows promise toward transparent, understandable, and explainable decision-making.
	    </p>
		   <br/><br/>
	      <hr>
     
   
   <img src="ghadazam.github.io/img/jp.jpg" style="float:left; margin:45px; border-radius:20%;">         
	    <b><a><h3> <b>Title: </b> Lessons learned from Machine Learning in Google applied to Medical and Biological Imaging</h3></a>
		 <a href="https://www.healthit.gov/hitac/member/ming-jack-po"><h3><b>Speaker:</b> Ming Jack Po, Google </h3></a> <p></b>
 	 <p style="text-align:left;"> 
	    In March of 2016, the AlphaGo computer program beat world champion (and human) Lee Sedol at the board game Go. The program's success reflected the significant progress that machine learning research has made in recent years. However, AlphaGo was just one example of what can be achieved with machine learning. This talk will provide an overview of some of the techniques that are being used in machine learning today, as well as some recent and ongoing work by Google's research teams to advance the applications of machine learning, particularly its role in biomedical research.  The talk will also discuss some of the unique challenges around applications in healthcare.
	    </p>
	    <br/><br/>
	    <hr>
     
         
   
   <img src="ghadazam.github.io/img/XueZhiyun.jpg" style="float:left; margin:45px; border-radius:20%;">         
	    <b><a><h3><b>Title: </b>Automated Visual Evaluation (AVE) for Cervical Cancer Screening and its Challenges </h3></a>
		    <a href="https://www.nlm.nih.gov/index.html"><h3><b>Speaker: </b>Zhiyun (Jaylene) Xue , National Library of Medicine </h3></a> <p></b>
	<p style="text-align:left;"> 
		Cervical cancer is one of the leading gynecologic diseases that affects the life of many women worldwide especially in low- and medium-resource regions (LMRR). Regular screening and early diagnosis and treatment play a critical role in the prevention of cervical cancer. Visual inspection with acetic acid (VIA) is an inexpensive screening approach commonly used in LMRR, but has been shown to be inaccurate. In this talk, we present our work on utilizing deep learning techniques to automatically evaluate the visual appearance of the acetowhitened uterine cervix using mobile devices for the goal of assisting or maybe ultimately replacing VIA. We face well-known challenges that impact use of deep learning for medical imaging applications, such as, having a small amount of labeled data or having a very unbalanced datasets. But, there are also additional practical challenges that need to be addressed for using AVE in real world, such as, the control of image quality and its influence on the AVE algorithm, the robustness of the algorithm across multiple devices, and the adjustment of AVE for variability in cervix appearance in different geographical regions. The talk will describe our findings and challenges that guide next steps for research in the field. 
	    </p>
	     <br/> <br/>
	    <hr>
     
   <img src="ghadazam.github.io/img/mz.jpeg" style="float:left; margin:45px; border-radius:20%;">         
	    <b><a><h3><b>Title: </b>Integrating Imaging, Omics, and Clinical Data towards Improved Outcome Management in Lung Cancer</h3></a>
		    <a href="https://muzhou1.github.io"><h3> <b>Speaker: </b>Mu Zhou, SenseBrain AI Technology </h3></a> <p></b>
	<p style="text-align:left;"> 
		Growing amounts of medical imaging and molecular data offer new opportunities to better understand cancer biology. In this talk, I will highlight ongoing progress on modeling multi-scale biomedical data for linking imaging, omics, and clinical data to advance our understanding in lung cancer. First, I will present recent works on linking CT images and RNA profiles in non-small cell lung cancer (NSCLC). We propose an image-to-genomics map to identify non-invasive biomarkers with prognostic implications by leveraging public gene expression cohorts. Second, I will discuss how we can develop efficient image-based deep learning classifiers to predict survival outcomes in lung cancer across multiple clinical centers. Ongoing clinical data challenges and opportunities will also be addressed in related areas. 
	    </p>
	     <br/> <br/>
	    <hr>
     
     <img src="ghadazam.github.io/img/sr.jpg" style="float:left; margin:45px; border-radius:20%;">         
	    <b><a><h3><b>Title: </b>Deep neural ensembles for improved pulmonary abnormality detection in chest radiographs </h3></a>
		    <a href="https://lhncbc.nlm.nih.gov/personnel/sivaramakrishnan-rajaraman"><h3> <b>Speaker: </b>Sivaramakrishnan Rajaraman, National Library of Medicine </h3></a> <p></b>
	<p style="text-align:left;"> 
		Cardiopulmonary diseases account for a significant proportion of deaths and disabilities across the world. Chest X-rays are a common diagnostic imaging modality for confirming intra-thoracic cardiopulmonary abnormalities. However, there remains an acute shortage of expert radiologists, particularly in under-resourced settings that results in interpretation delays and could have global health impact. These issues can be mitigated by an artificial intelligence (AI) powered computer-aided diagnostic (CADx) system. Such a system could help supplement decision-making and improve throughput while preserving and possibly improving the standard-of-care. A majority of such AI-based diagnostic tools at present use data-driven deep learning (DL) models that perform automated feature extraction and classification. Convolutional neural networks (CNN), a class of DL models, have gained significant research prominence in tasks related to image classification, detection, and localization. The literature reveals that they deliver promising results that scale impressively with an increasing number of training samples and computational resources. However,  the techniques may be adversely impacted due to their sensitivity to high variance or fluctuations in training data. Ensemble learning helps mitigate these by combining predictions  and blending intelligence from multiple learning algorithms. Complex non-linear functions constructed within ensembles help improve robustness and generalization. Empirical result predictions have demonstrated superiority over the conventional approach with stand-alone CNN models. In this talk, I will describe example work at the NLM that use model ensembles to improve pulmonary abnormality detection in chest radiographs.
	    </p>
	     <br/> <br/>
	    <hr>
     
     
     <img src="ghadazam.github.io/img/ar.jpeg" style="float:left; margin:45px; border-radius:20%;">         
	    <b><a><h3> <b>Title: </b>Intel AI Technologies for Efficient Medical Image Analysis </h3></a>
		 <a href="https://www.linkedin.com/in/skysurgery" target="_blank"><h3><b>Speaker:</b> Anthony Reina, Intel Corporation</h3></a> <p></b>
 	 <p style="text-align:left;"> 
		 Anthony will walk through how to optimize deep learning models using the Intel® Distribution of the OpenVINO toolkit. OpenVINO is an open-sourced software solution that allows developers to optimize their deep learning pipelines and get the maximum performance on Intel hardware. He’ll give an overview of several FDA-cleared deep learning models that are in deployment today in hospitals, give tips on how to design high-throughput, low-latency models, and cover different use cases applied to MRI and X-ray  medical images. 
	    </p>
	    <br/><br/>
	    <hr>
	    
	      
</div> 
</section>

	<br><br>
	
	<!--==========================
    Footer
  ============================-->
  <footer id="footer">
    <div class="footer-top">
      <div class="container">
        <div class="row">
		<!--
          <div class="col-lg-3 col-md-6 footer-info">
            <img src="img/logo.png" alt="TheEvenet">
            <p>In alias aperiam. Placeat tempore facere. Officiis voluptate ipsam vel eveniet est dolor et totam porro. Perspiciatis ad omnis fugit molestiae recusandae possimus. Aut consectetur id quis. In inventore consequatur ad voluptate cupiditate debitis accusamus repellat cumque.</p>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="#">Home</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">About us</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Services</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>-->

         <div class="col-lg-12 col-md-6 footer-contact">
	    <center> <h4>Questions?</h4> </center>
            <p>
            <center> <b>Contact: Ghada Zamzmi</b></center>
            <center> <b>alzamzmiga AT mail DOT nih DOT gov</b></center>
            </p>
           

          </div>

        </div>
      </div>
    </div>

    <div class="container">
      <div class="copyright">
        &copy; 2019-2020 <strong>NLM/EMBC2020 </strong> All Rights Reserved.
      </div>
      <div class="credits">
        <!--
          All the links in the footer should remain intact.
          You can delete the links only if you purchased the pro version.
          Licensing information: https://bootstrapmade.com/license/
          Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=TheEvent
        
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a> -->
      </div>
    </div>
  </footer><!-- #footer -->

   

  <a href="#" class="back-to-top"><i class="fa fa-angle-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/venobox/venobox.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>
</body>

</html>

